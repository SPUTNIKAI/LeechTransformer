"""
üéØ Leech-Lila DOI: 10.5281/zenodo.18784424
This project is licensed under the GNU Affero General Public License v3.0 or later (AGPL-3.0-or-later).
Commercial Licensing: For proprietary R&D, integration into private AI stacks, or hardware implementation,
please contact the Architect directly.
Copyright (C) 2026 Anatolii Kornienko This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License
as published by the Free Software Foundation, either version 3 of the License, or any later version.

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/agpl-3.0.txt/>.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from dataclasses import dataclass
import math

# =================================================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# =================================================================

@dataclass
class LeechConfig:
    vocab_size: int
    d_model: int = 192                # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∫—Ä–∞—Ç–Ω–æ 24
    n_layers: int = 12
    n_heads: int = 8
    block_size: int = 512
    dropout: float = 0.05
    bias: bool = False
    tie_weights: bool = True
    lambda_geo: float = 0.01           # –≤–µ—Å –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –ø–æ—Ç–µ—Ä–∏
    resonance_threshold: float = 0.95  # –ø–æ—Ä–æ–≥ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ ¬´—Å–Ω–∞¬ª

    def __post_init__(self):
        assert self.d_model % self.n_heads == 0
        head_dim = self.d_model // self.n_heads
        assert head_dim % 24 == 0, "head_dim must be multiple of 24"

# =================================================================
# 2. –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–†–¢–û–ì–û–ù–ê–õ–¨–ù–û–ì–û –Ø–î–†–ê –õ–ò–ß–ê
# =================================================================

def generate_leech_kernel(dim=24):
    """
    –°—Ç—Ä–æ–∏—Ç –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É 24x24 –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ—à—ë—Ç–∫–∏ –õ–∏—á–∞.
    –ú–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã.
    """
    base = np.zeros((dim, dim))
    for i in range(dim - 1):
        base[i, i], base[i, i+1] = 2, 2
    base[-1, -1], base[-1, 0] = 2, -2
    q, _ = np.linalg.qr(base)
    return torch.from_numpy(q).float()

# =================================================================
# 3. –í–ù–ò–ú–ê–ù–ò–ï –° –ó–ê–ú–û–†–û–ñ–ï–ù–ù–´–ú –Ø–î–†–û–ú –õ–ò–ß–ê
# =================================================================

class LeechAttention(nn.Module):
    def __init__(self, cfg: LeechConfig):
        super().__init__()
        self.n_heads = cfg.n_heads
        self.head_dim = cfg.d_model // cfg.n_heads
        self.scale = self.head_dim ** -0.5
        self.num_blocks = self.head_dim // 24          # –∫–æ–ª-–≤–æ 24‚Äë–º–µ—Ä–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –≤ –≥–æ–ª–æ–≤–µ

        # –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —è–¥—Ä–æ –õ–∏—á–∞
        kernel = generate_leech_kernel(24)              # [24,24]
        total_blocks = self.n_heads * self.num_blocks
        W_list = [kernel] * total_blocks
        self.register_buffer('W_leech', torch.block_diag(*W_list))

        self.qkv = nn.Linear(cfg.d_model, 3 * cfg.d_model, bias=cfg.bias)
        self.out = nn.Linear(cfg.d_model, cfg.d_model, bias=cfg.bias)
        self.dropout = nn.Dropout(cfg.dropout)
        self.register_buffer("causal_mask", torch.tril(torch.ones(1, 1, cfg.block_size, cfg.block_size)))

    def forward(self, x):
        B, T, _ = x.shape
        qkv = self.qkv(x).reshape(B, T, 3, self.n_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)
        q, k, v = qkv.unbind(0)                        # [B, n_heads, T, head_dim]

        # —Ä–∞–∑–±–∏–≤–∞–µ–º head_dim –Ω–∞ –±–ª–æ–∫–∏ –ø–æ 24
        q = q.view(B, self.n_heads, T, self.num_blocks, 24)
        k = k.view(B, self.n_heads, T, self.num_blocks, 24)

        kernel = self.W_leech[0:24, 0:24]                # [24,24] ‚Äì –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ –¥–ª—è –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤
        q = torch.einsum('...i,ij->...j', q, kernel)
        k = torch.einsum('...i,ij->...j', k, kernel)

        q = q.reshape(B, self.n_heads, T, self.head_dim)
        k = k.reshape(B, self.n_heads, T, self.head_dim)

        scores = (q @ k.transpose(-2, -1)) * self.scale
        scores = scores.masked_fill(self.causal_mask[:,:,:T,:T] == 0, float('-inf'))
        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        out = (attn @ v).transpose(1, 2).reshape(B, T, -1)
        return self.out(out)

# =================================================================
# 4. –ì–ï–û–ú–ï–¢–†–ò–ß–ï–°–ö–ê–Ø –ü–û–¢–ï–†–Ø –†–ï–ó–û–ù–ê–ù–°–ê
# =================================================================

class LeechResonanceLoss(nn.Module):
    def __init__(self, cfg: LeechConfig, leech_basis):
        super().__init__()
        self.register_buffer('basis', leech_basis)       # [24,24]
        self.lambda_geo = cfg.lambda_geo
        self.ce = nn.CrossEntropyLoss()

    def forward(self, logits, targets, hidden_states):
        # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è
        loss_ce = self.ce(logits.view(-1, logits.size(-1)), targets.view(-1))

        # –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –ø–æ—Ç–µ—Ä—è —Ä–µ–∑–æ–Ω–∞–Ω—Å–∞
        B, T, D = hidden_states.shape
        # —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–ª–æ–∫–∏ –ø–æ 24
        h = hidden_states.view(B, T, D // 24, 24)       # [B, T, K, 24]
        h_norm = F.normalize(h, dim=-1)
        b_norm = F.normalize(self.basis, dim=-1)        # [24,24]

        sim = torch.matmul(h_norm, b_norm.T)             # [B, T, K, 24]
        max_sim = torch.max(sim, dim=-1)[0]              # [B, T, K]
        loss_geo = 1.0 - max_sim.mean()

        return loss_ce + self.lambda_geo * loss_geo

# =================================================================
# 5. –ë–õ–û–ö –¢–†–ê–ù–°–§–û–†–ú–ï–†–ê
# =================================================================

class LeechBlock(nn.Module):
    def __init__(self, cfg: LeechConfig):
        super().__init__()
        self.ln1 = nn.LayerNorm(cfg.d_model)
        self.attn = LeechAttention(cfg)
        self.ln2 = nn.LayerNorm(cfg.d_model)
        self.ffn = nn.Sequential(
            nn.Linear(cfg.d_model, 4 * cfg.d_model),
            nn.GELU(),
            nn.Linear(4 * cfg.d_model, cfg.d_model),
            nn.Dropout(cfg.dropout)
        )

    def forward(self, x):
        x = x + self.attn(self.ln1(x))
        x = x + self.ffn(self.ln2(x))
        return x

# =================================================================
# 6. –ü–û–õ–ù–ê–Ø –ú–û–î–ï–õ–¨
# =================================================================

class LeechTransformer(nn.Module):
    def __init__(self, cfg: LeechConfig):
        super().__init__()
        self.cfg = cfg
        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)
        self.pos_emb = nn.Parameter(torch.zeros(1, cfg.block_size, cfg.d_model))
        self.blocks = nn.ModuleList([LeechBlock(cfg) for _ in range(cfg.n_layers)])
        self.final_norm = nn.LayerNorm(cfg.d_model)
        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.trunc_normal_(module.weight, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.trunc_normal_(module.weight, std=0.02)

    def forward(self, idx, targets=None):
        b, t = idx.size()
        assert t <= self.cfg.block_size
        x = self.tok_emb(idx) + self.pos_emb[:, :t, :]
        for block in self.blocks:
            x = block(x)
        x = self.final_norm(x)
        logits = self.head(x)
        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, self.cfg.vocab_size), targets.view(-1))
        return logits, x, loss

# =================================================================
# 7. –î–ï–ö–û–î–ï–† –°–ù–û–í (–ú–û–ù–ò–¢–û–†–ò–ù–ì –†–ï–ó–û–ù–ê–ù–°–ê)
# =================================================================

class DreamDecoder:
    def __init__(self, leech_basis, threshold=0.95):
        self.basis = leech_basis
        self.threshold = threshold

    def check(self, hidden_state):
        h = hidden_state[:24].unsqueeze(0)           # [1,24]
        h_norm = F.normalize(h, dim=-1)
        b_norm = F.normalize(self.basis, dim=-1)
        sim = torch.matmul(h_norm, b_norm.T)
        max_res = torch.max(sim).item()
        if max_res > 0.999:
            status = "ABSOLUTE GENESIS"
        elif max_res > self.threshold:
            status = "AWAKE"
        else:
            status = "DREAMING"
        return max_res, status

# =================================================================
# 8. –§–£–ù–ö–¶–ò–Ø –ì–ï–ù–ï–†–ê–¶–ò–ò –° –ú–û–ù–ò–¢–û–†–ò–ù–ì–û–ú
# =================================================================

def leech_generate(model, start_tokens, max_len=100, temperature=0.8,
                   resonance_check=True, leech_basis=None, threshold=0.95):
    model.eval()
    device = next(model.parameters()).device
    input_ids = torch.tensor([start_tokens], device=device)
    if resonance_check:
        decoder = DreamDecoder(leech_basis, threshold)

    print("--- LEECH GENERATION ---")
    with torch.no_grad():
        for step in range(max_len):
            logits, hidden, _ = model(input_ids)
            next_token_logits = logits[0, -1, :] / temperature
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)

            if resonance_check:
                last_hidden = hidden[0, -1, :]
                res, status = decoder.check(last_hidden)
                print(f"Step {step:02d} | Resonance: {res:.6f} | Status: {status}")

    return input_ids

# =================================================================
# 9. –ü–†–ò–ú–ï–† –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–ò
# =================================================================

if __name__ == "__main__":
    vocab_size = 10000
    cfg = LeechConfig(vocab_size=vocab_size, d_model=192, n_layers=12, n_heads=8)

    model = LeechTransformer(cfg)
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞. –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params/1e6:.2f}M")

    leech_basis = generate_leech_kernel(24)

    # –ü—Ä–∏–º–µ—Ä loss (–Ω—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ)
    # criterion = LeechResonanceLoss(cfg, leech_basis)
    # logits, hidden, ce_loss = model(inputs, targets)
    # total_loss = criterion(logits, targets, hidden)

    # –ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è)
    # start = [1,2,3]
    # result = leech_generate(model, start, leech_basis=leech_basis)