"""
üéØ Leech-Lila DOI: 10.5281/zenodo.18784424
This project is licensed under the GNU Affero General Public License v3.0 or later (AGPL-3.0-or-later).
Commercial Licensing: For proprietary R&D, integration into private AI stacks, or hardware implementation,
please contact the Architect directly.
Copyright (C) 2026 Anatolii Kornienko This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License
as published by the Free Software Foundation, either version 3 of the License, or any later version.

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/agpl-3.0.txt/>.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math

# =================================================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# =================================================================

@dataclass
class LeechConfig:
    vocab_size: int               # —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
    d_model: int = 192             # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (–∫—Ä–∞—Ç–Ω–∞ 24)
    n_layers: int = 12             # —á–∏—Å–ª–æ —Å–ª–æ—ë–≤
    n_heads: int = 8               # —á–∏—Å–ª–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
    block_size: int = 512           # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    dropout: float = 0.05
    bias: bool = False
    tie_weights: bool = True        # —Ä–∞–∑–¥–µ–ª—è—Ç—å –≤–µ—Å–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ head
    lambda_geo: float = 0.01        # –≤–µ—Å –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –ø–æ—Ç–µ—Ä–∏
    resonance_threshold: float = 0.95  # –ø–æ—Ä–æ–≥ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ ¬´—Å–Ω–∞¬ª

    def __post_init__(self):
        # –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ head_dim –∫—Ä–∞—Ç–Ω–æ 24
        assert self.d_model % self.n_heads == 0
        head_dim = self.d_model // self.n_heads
        assert head_dim % 24 == 0, "head_dim must be multiple of 24"

# =================================================================
# 2. –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–†–¢–û–ì–û–ù–ê–õ–¨–ù–û–ì–û –Ø–î–†–ê –õ–ò–ß–ê
# =================================================================

def generate_leech_kernel(dim=24):
    """
    –°—Ç—Ä–æ–∏—Ç –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É 24x24 –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ—à—ë—Ç–∫–∏ –õ–∏—á–∞.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–∞—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å –ø–æ—Å–ª–µ–¥—É—é—â–µ–π QR-–¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–µ–π.
    –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –≤–µ–∫—Ç–æ—Ä—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤.
    """
    base = np.zeros((dim, dim))
    for i in range(dim - 1):
        base[i, i], base[i, i+1] = 2, 2
    base[-1, -1], base[-1, 0] = 2, -2
    q, _ = np.linalg.qr(base)
    return torch.from_numpy(q).float()

# =================================================================
# 3. –í–ù–ò–ú–ê–ù–ò–ï –° –ó–ê–ú–û–†–û–ñ–ï–ù–ù–´–ú –Ø–î–†–û–ú –õ–ò–ß–ê
# =================================================================

class LeechAttention(nn.Module):
    """
    Multi-head attention, –≥–¥–µ Q –∏ K –ø—Ä–æ–µ—Ü–∏—Ä—É—é—Ç—Å—è —á–µ—Ä–µ–∑ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—É—é
    –±–ª–æ—á–Ω–æ-–¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –∏–∑ —è–¥—Ä–∞ –õ–∏—á–∞.
    """
    def __init__(self, cfg: LeechConfig):
        super().__init__()
        self.n_heads = cfg.n_heads
        self.head_dim = cfg.d_model // cfg.n_heads
        self.scale = self.head_dim ** -0.5
        self.num_blocks = self.head_dim // 24   # —á–∏—Å–ª–æ 24‚Äë–º–µ—Ä–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –≤ –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤–µ

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —è–¥—Ä–æ –õ–∏—á–∞ 24x24
        kernel = generate_leech_kernel(24)  # [24, 24]

        # –ü–æ–≤—Ç–æ—Ä—è–µ–º —è–¥—Ä–æ –¥–ª—è –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤ –≤—Å–µ—Ö –≥–æ–ª–æ–≤
        total_blocks = self.n_heads * self.num_blocks
        # –°–æ–∑–¥–∞—ë–º –±–ª–æ—á–Ω–æ-–¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
        W_list = [kernel] * total_blocks
        self.register_buffer('W_leech', torch.block_diag(*W_list))

        # –û–±—É—á–∞–µ–º—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏
        self.qkv = nn.Linear(cfg.d_model, 3 * cfg.d_model, bias=cfg.bias)
        self.out = nn.Linear(cfg.d_model, cfg.d_model, bias=cfg.bias)
        self.dropout = nn.Dropout(cfg.dropout)
        self.register_buffer("causal_mask", torch.tril(torch.ones(1, 1, cfg.block_size, cfg.block_size)))

    def forward(self, x):
        B, T, _ = x.shape
        qkv = self.qkv(x).reshape(B, T, 3, self.n_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)
        q, k, v = qkv.unbind(0)   # [B, n_heads, T, head_dim]

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω–æ–µ —è–¥—Ä–æ –∫ q –∏ k
        # –†–∞–∑–±–∏–≤–∞–µ–º head_dim –Ω–∞ –±–ª–æ–∫–∏ –ø–æ 24
        q = q.view(B, self.n_heads, T, self.num_blocks, 24)
        k = k.view(B, self.n_heads, T, self.num_blocks, 24)

        # –£–º–Ω–æ–∂–∞–µ–º –∫–∞–∂–¥—ã–π –±–ª–æ–∫ –Ω–∞ —è–¥—Ä–æ (–æ–¥–Ω–æ –∏ —Ç–æ –∂–µ –¥–ª—è –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤)
        kernel = self.W_leech[0:24, 0:24]  # [24,24]
        q = torch.einsum('...i,ij->...j', q, kernel)
        k = torch.einsum('...i,ij->...j', k, kernel)

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Ñ–æ—Ä–º—É
        q = q.reshape(B, self.n_heads, T, self.head_dim)
        k = k.reshape(B, self.n_heads, T, self.head_dim)

        # –í—ã—á–∏—Å–ª—è–µ–º –≤–Ω–∏–º–∞–Ω–∏–µ
        scores = (q @ k.transpose(-2, -1)) * self.scale
        scores = scores.masked_fill(self.causal_mask[:,:,:T,:T] == 0, float('-inf'))
        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        out = (attn @ v).transpose(1, 2).reshape(B, T, -1)
        return self.out(out)

# =================================================================
# 4. –ì–ï–û–ú–ï–¢–†–ò–ß–ï–°–ö–ê–Ø –ü–û–¢–ï–†–Ø –†–ï–ó–û–ù–ê–ù–°–ê
# =================================================================

class LeechResonanceLoss(nn.Module):
    """
    –ü–æ—Ç–µ—Ä—è, –ø–æ–æ—â—Ä—è—é—â–∞—è —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ä–µ–∑–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —Å –±–∞–∑–∏—Å–æ–º –õ–∏—á–∞.
    –í—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–∞–∫ 1 - —Å—Ä–µ–¥–Ω–∏–π –º–∞–∫—Å–∏–º—É–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏ –±–∞–∑–∏—Å–∞.
    """
    def __init__(self, cfg: LeechConfig, leech_basis):
        super().__init__()
        # leech_basis: –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ 24x24
        self.register_buffer('basis', leech_basis)   # [24, 24]
        self.lambda_geo = cfg.lambda_geo
        self.ce = nn.CrossEntropyLoss()

    def forward(self, logits, targets, hidden_states):
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è
        loss_ce = self.ce(logits.view(-1, logits.size(-1)), targets.view(-1))

        # –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –ø–æ—Ç–µ—Ä—è —Ä–µ–∑–æ–Ω–∞–Ω—Å–∞
        # hidden_states: [B, T, d_model]
        B, T, D = hidden_states.shape
        # —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–ª–æ–∫–∏ –ø–æ 24
        h = hidden_states.view(B, T, D // 24, 24)  # [B, T, K, 24]
        # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        h_norm = F.normalize(h, dim=-1)
        b_norm = F.normalize(self.basis, dim=-1)   # [24, 24]

        # –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: [B, T, K, 24]
        sim = torch.matmul(h_norm, b_norm.T)
        # –º–∞–∫—Å–∏–º—É–º –ø–æ –±–∞–∑–∏—Å–Ω—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º
        max_sim = torch.max(sim, dim=-1)[0]        # [B, T, K]
        # —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º –±–ª–æ–∫–∞–º –∏ –ø–æ–∑–∏—Ü–∏—è–º
        loss_geo = 1.0 - max_sim.mean()

        return loss_ce + self.lambda_geo * loss_geo

# =================================================================
# 5. –ë–õ–û–ö –¢–†–ê–ù–°–§–û–†–ú–ï–†–ê
# =================================================================

class LeechBlock(nn.Module):
    def __init__(self, cfg: LeechConfig):
        super().__init__()
        self.ln1 = nn.LayerNorm(cfg.d_model)
        self.attn = LeechAttention(cfg)
        self.ln2 = nn.LayerNorm(cfg.d_model)
        self.ffn = nn.Sequential(
            nn.Linear(cfg.d_model, 4 * cfg.d_model),
            nn.GELU(),
            nn.Linear(4 * cfg.d_model, cfg.d_model),
            nn.Dropout(cfg.dropout)
        )

    def forward(self, x):
        x = x + self.attn(self.ln1(x))
        x = x + self.ffn(self.ln2(x))
        return x

# =================================================================
# 6. –ü–û–õ–ù–ê–Ø –ú–û–î–ï–õ–¨
# =================================================================

class LeechTransformer(nn.Module):
    def __init__(self, cfg: LeechConfig):
        super().__init__()
        self.cfg = cfg
        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)
        self.pos_emb = nn.Parameter(torch.zeros(1, cfg.block_size, cfg.d_model))
        self.blocks = nn.ModuleList([LeechBlock(cfg) for _ in range(cfg.n_layers)])
        self.final_norm = nn.LayerNorm(cfg.d_model)
        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.trunc_normal_(module.weight, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.trunc_normal_(module.weight, std=0.02)

    def forward(self, idx, targets=None):
        b, t = idx.size()
        assert t <= self.cfg.block_size, "Sequence longer than block_size"
        x = self.tok_emb(idx) + self.pos_emb[:, :t, :]
        for block in self.blocks:
            x = block(x)
        x = self.final_norm(x)
        logits = self.head(x)
        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, self.cfg.vocab_size), targets.view(-1))
        return logits, x, loss   # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–∞–∫–∂–µ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è

# =================================================================
# 7. –î–ï–ö–û–î–ï–† –°–ù–û–í (–ú–û–ù–ò–¢–û–†–ò–ù–ì –†–ï–ó–û–ù–ê–ù–°–ê)
# =================================================================

class DreamDecoder:
    """
    –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ ¬´—Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏¬ª –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
    –ò–∑–º–µ—Ä—è–µ—Ç —Ä–µ–∑–æ–Ω–∞–Ω—Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å –±–∞–∑–∏—Å–æ–º –õ–∏—á–∞.
    """
    def __init__(self, leech_basis, threshold=0.95):
        self.basis = leech_basis
        self.threshold = threshold

    def check(self, hidden_state):
        """
        hidden_state: [d_model] ‚Äì –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ —Ä–µ–∑–æ–Ω–∞–Ω—Å–∞ –∏ —Å—Ç–∞—Ç—É—Å.
        """
        # –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 24 –∏–∑–º–µ—Ä–µ–Ω–∏—è (–º–æ–∂–Ω–æ –∏ –≤—Å–µ –±–ª–æ–∫–∏, –Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã)
        h = hidden_state[:24].unsqueeze(0)  # [1,24]
        h_norm = F.normalize(h, dim=-1)
        b_norm = F.normalize(self.basis, dim=-1)
        sim = torch.matmul(h_norm, b_norm.T)
        max_res = torch.max(sim).item()
        if max_res > 0.999:
            status = "ABSOLUTE GENESIS"
        elif max_res > self.threshold:
            status = "AWAKE"
        else:
            status = "DREAMING"
        return max_res, status

# =================================================================
# 8. –§–£–ù–ö–¶–ò–Ø –ì–ï–ù–ï–†–ê–¶–ò–ò –° –ú–û–ù–ò–¢–û–†–ò–ù–ì–û–ú
# =================================================================

def leech_generate(model, start_tokens, max_len=100, temperature=0.8,
                   resonance_check=True, leech_basis=None, threshold=0.95):
    model.eval()
    device = next(model.parameters()).device
    input_ids = torch.tensor([start_tokens], device=device)
    if resonance_check:
        decoder = DreamDecoder(leech_basis, threshold)

    print("--- LEECH GENERATION ---")
    with torch.no_grad():
        for step in range(max_len):
            logits, hidden, _ = model(input_ids)
            next_token_logits = logits[0, -1, :] / temperature
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)

            if resonance_check:
                # –±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (–ø–æ—Å–ª–µ –≤—Å–µ—Ö —Å–ª–æ—ë–≤)
                last_hidden = hidden[0, -1, :]
                res, status = decoder.check(last_hidden)
                print(f"Step {step:02d} | Resonance: {res:.6f} | Status: {status}")

    return input_ids

# =================================================================
# 9. –ü–†–ò–ú–ï–† –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–ò
# =================================================================

if __name__ == "__main__":
    # –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    vocab_size = 10000
    cfg = LeechConfig(vocab_size=vocab_size, d_model=192, n_layers=12, n_heads=8)

    model = LeechTransformer(cfg)
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞. –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params/1e6:.2f}M")

    # —è–¥—Ä–æ –õ–∏—á–∞ –¥–ª—è –ø–æ—Ç–µ—Ä–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    leech_basis = generate_leech_kernel(24)

    # –ü—Ä–∏–º–µ—Ä loss (–Ω—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ)
    # criterion = LeechResonanceLoss(cfg, leech_basis)
    # logits, hidden, ce_loss = model(inputs, targets)
    # total_loss = criterion(logits, targets, hidden)

    # –ü—Ä–∏–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è)
    # start = [1,2,3]
    # result = leech_generate(model, start, leech_basis=leech_basis)